% Template for PLoS
% Version 3.4 January 2017
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that 
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% Leave date blank
\date{}

% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{27.023pt}
\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\sf PLOS}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Amquery: a unified searchable database of 16S rRNA amplicon libraries} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\


Nikolay Romashchenko \textsuperscript{1 $\ast$},
Ilia Korvigo \textsuperscript{1, 2},
Evgeny Andronov \textsuperscript{1},

\bigskip
\textbf{1} Laboratory of Microbiological Monitoring and Bioremediation of Soils, All-Russia Research Institute for Agricultural Microbiology, St. Petersburg, Russia
\\
\textbf{2} Laboratory of Functional Genomics, Moscow Institute of Physics and Technology, Moscow, Russia
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
%\Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.
%\ddag These authors also contributed equally to this work.

% Current address notes
%\textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% \textcurrency b Insert second current address 
% \textcurrency c Insert third current address

% Deceased author note
%\dag Deceased

% Group/Consortium Author Note
%\textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* nikolay.romashchenko@gmail.com

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
Rapidly increasing amounts of 16S amplicon libraries require the development of efficient methods for similarity search. 
Here we present Amquery, a new tool for fast indexing and similarity search of 16S rRNA amplicon libraries, based on a technique of k-mer abundance comparison.
Experimental results on huge 16S dataset show that Amquery can significantly outperform clustering-based pipelines, implemented in tools such as QIIME, in the index construction and search tasks. 
Amquery is freely available at https://github.com/nromashchenko/amquery.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
Deep high-throughput amplicon sequencing has become the de-facto standard method for taxonomic community profiling in environmental, 
medical and industrial microbiology, because it provides cheap high-resolution data in a snap \cite{}. Consequently, the 
number of samples deposited in public databases, such as NCBI SRA and MG-RAST, keeps growing at a hyper-linear rate, 
which resembles the period of rapid accumulation of individual sequences in the NCBI GenBank database. 
And as much as rapid growth of the GenBank drove the development of fast and efficient tools for database-wide 
sequence comparison (i.e. FASTA and, subsequently, BLAST) \cite{}, the ever-growing number of publicly available amplicon 
libraries necessitates a mean to query the databases for similar samples without relying on available metadata, which is 
often scarce, incomplete and hard to quantify. Yet, several factors make this task seem computationally intractable within 
the conventional analytical framework and available software. A standard workflow (e.g. implemented in QIIME \cite{}) 
for comparing amplicon libraries usually comprises several major steps: 
1. library demultiplexing, 2. OTU-picking, 3. beta-diversity estimation. Library demultiplexing (mostly adapter and primer 
trimming) is probably the least computationally intensive and straightforward to run on multiple processing units in parallel, 
hence we are not going to get into details on that. 

On the other hand, OTU-picking is one of the most computationally intensive steps, coming in three major flavours: 
de-novo, closed-reference and open-reference picking (the latter is a combination of the other). 
Since amplicon sequencing is all about taxonomic screening of a community, OTU-picking algorithms group similar 
sequences into clusters that can be regarded as formally defined quasi-taxonomic entities dubbed OTUs (Operational 
Taxonomic Units). The clustering compensates for sequencing errors and, most importantly, makes standard downstream 
analyses more biologically sound and computationally tractable. In de-novo OTU-picking clusters are created from scratch 
without any prior information, which, if done accurately, becomes unfeasible for any realistically sized amplicon libraries, 
giving way for efficient and heavily heuristic algorithms, such as UCLAST \cite{} and CD-HIT \cite{}. In closed-reference picking 
no clusters are created from scratch – instead, a user provides a set of reference sequences to use as cluster seeds. 
Any sequence, failing to hit a given similarity threshold to any reference, is discarded. In open-reference picking 
these discarded sequences are subjected to de-novo clustering. Reference-based picking is easier to run on multiple 
processing units and is less prone to artefacts, rooted in the aggressive heuristics of fast sequence alignment and 
clustering tools. Anyway, after an algorithm/pipeline finishes the clustering, it picks a single representative sequence 
out of each cluster (in closed-reference picking the references are the representatives). This representative set can be 
used for downstream analyses, such as taxonomic identification and phylogenetics. The set also makes the results of 
de-novo and open-reference picking reproducible and stable, because it can be supplied to a closed-reference OTU-picking 
pipeline \cite{}.

After picking is over, each amplicon library can be represented as a count-vector, giving the per-OTU number of 
observations/reads. A matrix of such vectors (knows as the OTU-table) represents a set of microbial communities. 
To compare them, common pipelines calculate a square pair-wise dissimilarity matrix (aka the beta-diversity matrix), 
using a beta-diversity measure, most notably one of the UniFrac family [7]. These dissimilarity measures leverage 
phylogenetic information by projecting the communities onto a phylogenetic tree of its inhabitants, hence all variations 
of UniFrac require a tree of cluster representatives.

The preface immediately introduces two main issues a searchable database would need to address: OTU-picking and 
similarity estimation. First, we must choose a viable OTU-picking strategy or find a way to around it altogether. 
The hyper-linear complexity and, even more so, the clustering instability of de-novo and open-reference pipelines 
strike them out of the options to consider, leaving a single method on the list – the closed-reference picking, 
which requires a set of representatives. As self-obvious as it may seem, to make downstream beta-diversity estimations 
unbiased, the set must be representative of the expected cumulative diversity of all samples that can be added to the 
database. Additionally, since the overwhelming majority of amplicon libraries comprise short-reads targeting various 
regions in a marker gene (e.g. V1-V9 hypervariable regions of the 16S rRNA gene), complete-gene references are especially 
valuable, because they allow read-mapping-based picking, mitigating incompatibility of amplicon libraries generated 
for different regions of the same gene. While extensive reference sets of full-length marker genes exist for some 
major fields of microbiology (most notably, the HITdb 16S rRNA reference set designed for medical biology), in many 
cases such a set is yet to be created (e.g. for aquatic or soil environmental microbiology). Nevertheless, an 
exhaustive de-novo clustering based on hundreds or thousands of communities sampled from a specific environment (e.g. soil) 
n produce a set of references to use in a database until better alternatives become available. It’s important to stress, 
that the lack of full-length references makes it nearly impossible to create a unified searchable database of 
short-read libraries targeting different regions of the same gene. Therefore, in the absence of full-length 
references a searchable database would have to provide several subsections based on the commonly targeted regions. 
To use UniFrac for downstream similarity estimations, a phylogenetic tree would have to be inferred for each set of 
representatives, which can be computationally unachievable via accurate methods, because even a single set of 
communities (sampled from the same environment) often harbour many thousands of clusters. Furthermore, fast sequence 
clustering algorithms, such as UCLUST, tend to create spurious clusters, inflating representative sets, which not only 
affects different beta-diversity indices, but makes it increasingly hard to create a phylogenetic tree.


\section*{Methods}
Amquery implements a framework for amplicon libraries metric indexing written in Python, 
providing support for user-defined dissimilarity metrics. Such definitions consist of two major components:
1. a sample representation routine
2. a metric community dissimilarity measure.


\subsection*{Metric indexing storage}
In order to provide efficient distance metric based retrieval of amplicon library samples, 
Amquery implements a heavily researched concept of metric indexing. Generally speaking, metric indexing 
is a preprocessing of data designed to provide efficient metric based similarity search 
(see \cite{hetland2009basic} for more detailed overview).

Amquery utilises vantage-point tree (vp-tree) \cite{yianilos1993data, chavez2001searching} as an indexing 
data structure, which refers to so-called pivot-based metric indexing.
In short, pivot-based metric indexing exploits the idea of sequential partitioning of origin metric 
space, reducing the cost of traversing across the metric space during similarity search query. 
To index a set of input samples $S$, one creates a tree node and randomly picks a 'pivot' sample $p \in S$, calculating dissimilarity 
values between all pairs of the form $(p, x), x \in S \setminus \{ p \}$. 0
A median of the list of all of these values is a vantage-point for current tree node, which splits the list into two parts. Samples according to left and right part are indexed recursively in left and right subtree of current tree node, respectively.
To search for most similar samples of input sample $s$, one traverses throw the tree in the following way. Let $p$ to be a pivot sample of current tree node, $f$ to be a dissimilarity function and $k$ to be a number of most similar samples to search.
Then $f(s, p)$ is compared to corresponding median value, stored in current tree node, in order to choose a subtree for further traversing. If this subtree stores less than $k$ samples (and in some other cases), samples from alternative subtree are also processed in the same way. It is convenient to implement search routine using a queue to track candidate samples and returning first $k$ elements of the queue as a result list.

Since vantage-point tree is designed to be independent of the space dimensionality \cite{yianilos1993data}, this allows to store high-dimensional data with virtually no loss of indexing and search performance: it takes $\mathcal{O}(n)$ space and $\mathcal{O}(n \log n)$ time in the worst case to be built (for an input set of $n$ samples), and a search query is argued to require $\mathcal{O}(\log n)$ of time with some reservations. For more detailed analysis of vp-tree performance, see \cite{yianilos1993data}.


\subsection*{Dissimilarity metrics}
Amquery supports weighted UniFrac \cite{lozupone2011unifrac} as a community dissimilarity metric,
 which requires OTU-picking and phylogenetic tree construction on a precalculation stage. In this case, 
 samples are represented as vectors of OTU abundances (i.e. rows of OTU-table), which are directly used 
 in the calculation of pairwise distance values.


Since it can be computationally expensive to use OTU-based dissimilarity metrics, such as weighted UniFrac, Amquery 
implements an OTU-picking free dissimilarity metric over samples k-mer frequency distributions. Vectors of such distributions
are known as feature frequency profiles (FFP) and has been used in whole-genome comparison \cite{sims2009alignment}. 
Amquery utilises the square root of \textit{Jensen-Shannon divergence} (RJSD) \cite{lin1991divergence} that 
satisfies many statistical properties and fits many cases of high-dimensional spaces in $\mathbb{R^\star}$ as a distance 
metric \cite{fuglede2004jensen, endres2003new}. We make use of the specific form of RJSD, introduced 
by Connor et al. \cite{connor2013evaluation}, to speed up distance evaluation over sparse $k$-mer distributions (i.e. sparse FFP).


\subsubsection*{k-mer counting}
In the case of using RJSD as dissimilarity metric, Amquery represents an arbitrary amplicon library as a discrete 
probability distribution over the corresponding $k$-mer space (for a word-length $k$). 
To reduce memory consumption, the $k$-mers are encoded as quaternary values. 
More formally, each $k$-mer in a sequence $s = s_0,\dots,s_{n-1}$ is represented as its \textit{lexicographic rank} $\mathrm{lxr}_k(s)$, corresponding to the $k$-mer's position in the lexicographically sorted list of all possible $k$-mers.

\begin{eqnarray}
\label{eq:schemeP}
    \mathrm{lxr}_k(s) = \sum_{i=0}^{k−1} \mathrm{ord}(s_i) \cdot |{\mathcal{L}}|^{k−i−1}
\end{eqnarray}
Here $\mathrm{ord}(s_i)$ is a value of ordering function at $s_i$ over the alphabet ${\mathcal{L}} = \{A, C, G, T \}$. The most important advantage of this representation is that $\mathrm{lxr}_k$ is a special case of the Karp-Rabin hash fingerprint \cite{karp1987efficient}, making it possible to map $k$-mer $s_{i:i+k-1}$ to quaternary value $\mathrm{lxr}_k(s_{i:i+k-1})$, given the previous $k$-mer $s_{i−1:i+k-2}$ and its quaternary $\mathrm{lxr}_k(s_{i−1:i+k-2})$, in constant time:

\begin{eqnarray}
\label{eq:schemeP}
    \mathrm{lxr}_k(s_{i:i+k-1}) = |{\mathcal{L}}| \cdot (\mathrm{lxr}_k(s_{i−1:i+k-2}) − ord(s_{i−1}) \cdot 
									 |{\mathcal{L}}|^{k−1}) + ord(s_{i+k−1})
\end{eqnarray}
This advantage, however, imposes an upper-bound on the word-length $k$, since the number of possible $k$-mers grows exponentially with increasing word-length. That is, for a $k$-mer value to fit in a single machine word, we assume $k \leq 32$ for x64 architectures.
Since realistic sample ${k}$-mer distribution vectors are extremely high-dimensional and sparse, Amquery represents them as rank-sorted sparse arrays. In our implementation, a sparse array is a key-value storage in which the keys are the lexicographic ranks and the values are the empirical probabilities (i.e. relative abundances of the corresponding k-mers).




\section*{Results}

The weighted UniFrac based implementation of Amquery was compared against the RJSD based one in the following fashion.
We used the classic microbiome analysis QIIME \cite{caporaso2010qiime} pipeline to perform both a reference-based and \textit{de novo} OTU picking, 
and phylogenetic tree construction for huge 16S metagenomic dataset.
Resulting data were used for evaluation of weighted UniFrac distances during an index construction. 
Nearest neighbor search results for weighted UniFrac based implementation were used as a reference to estimate 
search relevance of the RJSD based one.

We also compared speed and memory efficiency of index construction between these two implementations.
All of the experiments were run on a machine equipped with an Intel Xeon CPU E5-2650 v3 (2.30 GHz)
 and a Seagate Constellation ES.3 ST3000NM0033 hard drive.

\subsection*{Data}
We analyzed a human gut 16S metagenomic dataset, which consists of 1978 samples, obtained from the Sequence Read Archive \cite{leinonen2010sequence}. 
For each sample, the reads were filtered by length with 250 base pairs threshold and rarefied by choosing 1000 sequences per sample randomly.
Resulting samples are split into two parts: referred below to as \texttt{main}, the size of which varies in range (100, 200, $\dots$, 1000), and referred below to as \texttt{additional}, 
consist of all the rest samples, accordingly. Resulting pairs of (\texttt{main}, \texttt{additional}) sample subsets were used in further processing.

\subsection*{Data processing}

\subsubsection*{By QIIME}
We used QIIME versions 1.9.1 to perform OTU picking and phylogenetic tree construction for the \texttt{main} part of every dataset split.
Reference-based and \textit{de novo} OTU picking were done using the default options as implemented in QIIME at the 95\% sequence similarity threshold, 
except for \texttt{uclust\_ref} OTU picking method for the reference-based case.

\subsubsection*{By Amquery}
For every split of the dataset, Amquery index was built for the \texttt{main} part, using weighted UniFrac and RJSD over FFPs ($k=15$ as k-mer size) as dissimilarity metrics.
Then, 100 random samples from the \texttt{main} part were searched in resulting index, and a random subset of the \texttt{additional} sample set of size 100 was added to the index. 
At every stage of processing, peak RAM usage and computational time were measured.

\subsection*{Speed and memory efficiency}
Elapsed time measurements at each stage of processing both for reference-based and \textit{de novo} QIIME pipeline against Amquery are presented in Fig~\ref{fig1} and Fig~\ref{fig2} respectively.
More detailed view of these measurements is presented Table~\ref{table1} and Table~\ref{table2},
where rows present measurements, made for each of the dataset splits, and the columns present elapsed time at each stage of pipelines. 
Similarly, peak RAM usage presented in Fig~\ref{fig3} and Fig~\ref{fig4} graphically and in Table~\ref{table3} and Table~\ref{table4} in numbers.


\begin{figure}[!h]
\caption{{\bf Elapsed time for QIIME (reference-based OTU picking) and Amquery pipelines for sets of samples of different size.}
A, B: Measurements of elapsed time, spended for reference-based OTU picking and beta-diversity analysis by QIIME using Bray-Curtis similarity (A) and weighted Unifrac (B). 
C: Measurements of elapsed time, spended for RJSD metric indexing by Amquery.}
\label{fig1}
\end{figure}

\begin{figure}[!h]
\caption{{\bf Elapsed time for QIIME (\textit{de novo} OTU picking) and Amquery pipelines for sets of samples of different size.}
A, B: Measurements of elapsed time, spended for \textit{de novo} OTU picking and beta-diversity analysis by QIIME using Bray-Curtis similarity (A) and weighted Unifrac (B). 
C: Measurements of elapsed time, spended for RJSD metric indexing by Amquery. Note that the C subfigure is the Fig~\ref{fig1}.C subfigure in the scale of A and B measurements. }
\label{fig2}
\end{figure}

\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{{\bf Measurements of processing time (mm:ss.ms) of Amquery and reference-based QIIME pipelines }}
\begin{tabular}{|l+l|l|l|l|l|l|l|l|l|}
\hline
\textit{size} & \multicolumn{3}{|l|}{\bf Amquery} & \multicolumn{6}{|l|}{\bf Reference-based QIIME}\\ \hline
& k-mers & indexing & {\bf Total} & pick\_otus & make\_otu & bdiv\_wu & bdiv\_bc & {\bf Total (wu)} & {\bf Total (bc)}  \\ \thickhline
100 & 0:43.08 & 0:00.74 & {\bf 0:43.82 } & 0:56.55 & 0:01.33 & 0:01.46 & 0:01.52 & {\bf 0:59.34} & {\bf 0:59.40} \\ \hline
200 & 1:25.16 & 0:01.51 & {\bf 1:26.67 } & 1:59.69 & 0:01.41 & 0:02.79 & 0:03.31 & {\bf 2:03.89} & {\bf 2:04.41} \\ \hline
300 & 2:10.24 & 0:02.40 & {\bf 2:12.64 } & 2:53.94 & 0:01.59 & 0:05.09 & 0:07.61 & {\bf 3:00.62} & {\bf 3:03.14} \\ \hline
400 & 2:44.32 & 0:03.13 & {\bf 2:47.45 } & 3:41.92 & 0:01.40 & 0:08.56 & 0:14.67 & {\bf 3:51.88} & {\bf 3:57.99} \\ \hline
500 & 3:37.39 & 0:04.31 & {\bf 3:41.70 } & 5:33.65 & 0:01.77 & 0:14.82 & 0:28.01 & {\bf 5:50.24} & {\bf 6:03.43} \\ \hline
600 & 4:17.47 & 0:05.45 & {\bf 4:22.92 } & 6:42.26 & 0:01.98 & 0:21.03 & 0:46.02 & {\bf 7:05.27} & {\bf 7:30.26} \\ \hline
700 & 5:08.55 & 0:06.00 & {\bf 5:14.55 } & 7:52.58 & 0:02.15 & 0:28.10 & 1:08.47 & {\bf 8:22.83} & {\bf 9:03.20} \\ \hline
800 & 5:38.59 & 0:07.07 & {\bf 5:45.66 } & 9:30.34 & 0:02.25 & 0:39.75 & 1:40.54 & {\bf 10:12.34} & {\bf 11:13.13} \\ \hline
900 & 6:33.67 & 0:07.83 & {\bf 6:41.50 } & 10:55.35 & 0:02.47 & 0:50.01 & 2:22.95 & {\bf 11:47.83} & {\bf 13:20.77} \\ \hline
1000 & 7:15.78 & 0:09.25 & {\bf 7:25.03 } & 11:54.37 & 0:02.54 & 1:04.56 & 2:58.86 & {\bf 13:01.47} & {\bf 14:55.77} \\ \hline
\end{tabular}
\begin{flushleft}
Interpretation of headings: \textit{size} is the number of samples, used for index construction. 
\textit{k-mers} is the time spent on k-mer counting.
\textit{indexing} is the time spent on vp-tree construction, including calculation of necessary RJSD distance values.
\textit{Total} is the total time of processing by Amquery.
\textit{pick\_otus} and \textit{make\_otu} are the total time spent by the \texttt{pick\_otus.py} and the \texttt{make\_otu\_table.py} scripts accordingly.
\textit{bdiv\_wu} and \textit{bdiv\_bc} are the total time spent by the \texttt{beta\_diversity.py} script for weighted Unifrac and Bray-Curtis similarities accordingly.
\textit{Total (wu)} and \textit{Total (bc)} are the total time spent by the QIIME pipeline for weighted Unifrac and Bray-Curtis similarities accordingly.
\end{flushleft}
\label{table1}
\end{adjustwidth}
\end{table}



\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{{\bf Measurements of processing time (mm:ss.ms or hh:mm:ss.ms) of \textit{de novo} based QIIME pipeline }}
\begin{tabular}{|l+l|l|l|l|l|l|l|l|l|l|}
\hline
\textit{size} & \multicolumn{10}{|l|}{\bf \textit{De novo} based QIIME}\\ \hline
& pick\_otus & rep\_set & align & filt\_aln & make\_phyl & make\_otu & bdiv\_wu & bdiv\_bc & {\bf Total (wu)} & {\bf Total (bc)}  \\ \thickhline
100 & 0:30.08 & 0:02.73 & 8:16.56 & 0:26.63 & 2:48.41 & 0:01.29 & 0:02.48 & 0:01.28 & {\bf 12:08.18} & {\bf 12:06.98} \\ \hline
200 & 1:03.83 & 0:04.11 & 15:33.94 & 0:46.64 & 5:10.33 & 0:01.46 & 0:07.04 & 0:03.14 & {\bf 22:47.35} & {\bf 22:43.45} \\ \hline
300 & 1:48.02 & 0:05.69 & 21:56.02 & 1:15.08 & 8:42.55 & 0:01.38 & 0:21.08 & 0:06.89 & {\bf 34:09.82} & {\bf 33:55.63} \\ \hline
400 & 2:25.44 & 0:06.97 & 26:42.65 & 1:26.78 & 11:38.27 & 0:01.52 & 0:40.01 & 0:13.04 & {\bf 43:01.64} & {\bf 42:34.67} \\ \hline
500 & 3:33.26 & 0:08.66 & 36:45.55 & 2:01.01 & 17:12.79 & 0:01.82 & 1:22.73 & 0:25.17 & {\bf 01:01:05.82} & {\bf 01:00:08.26} \\ \hline
600 & 4:38.02 & 0:10.59 & 42:20.45 & 2:26.52 & 22:15.39 & 0:01.75 & 2:22.34 & 0:42.02 & {\bf 01:14:15.06} & {\bf 01:12:34.74} \\ \hline
700 & 5:35.56 & 0:11.72 & 48:38.01 & 2:37.39 & 24:16.03 & 0:01.90 & 3:28.34 & 1:04.51 & {\bf 01:24:48.95} & {\bf 01:22:25.12} \\ \hline
800 & 7:03.39 & 0:13.10 & 55:14.31 & 2:55.26 & 27:43.87 & 0:02.19 & 7:49.36 & 1:32.94 & {\bf 01:41:01.48} & {\bf 01:34:45.06} \\ \hline
900 & 8:16.08 & 0:14.90 & 1:00:22 & 3:32.20 & 32:22.70 & 0:02.25 & 7:31.52 & 2:09.78 & {\bf 01:52:21.65} & {\bf 01:46:59.91} \\ \hline
1000 & 9:08.44 & 0:16.10 & 1:01:21 & 3:42.98 & 36:39.14 & 0:02.41 & 9:46.51 & 2:52.90 & {\bf 02:00:56.58} & {\bf 01:54:02.97} \\ \hline
\end{tabular}
\begin{flushleft}

\textit{pick\_otus}, \textit{make\_otu}, \textit{bdiv\_wu}, \textit{bdiv\_bc}, \textit{Total (wu)} and \textit{Total (bc)} columns have the same meanings as in the Table~\ref{table1}.
\textit{rep\_set}, \textit{align}, \textit{filt\_aln} and \textit{make\_phyl} contain the total time spent by the
\texttt{pick\_rep\_set.py}, \texttt{align\_seqs.py}, \texttt{filter\_alignment.py} and \texttt{make\_phylogeny.py} scripts accordingly.

\end{flushleft}
\label{table2}
\end{adjustwidth}
\end{table}



\begin{figure}[!h]
\caption{{\bf Peak RAM Usage for QIIME (reference-based OTU picking) and Amquery pipelines for sets of samples of different size.}
A, B: Peak RAM usage during reference-based OTU picking and beta-diversity analysis by QIIME using Bray-Curtis similarity (A) and weighted Unifrac (B). 
C: Peak RAM usage during sample indexing by Amquery.}
\label{fig3}
\end{figure}

\begin{figure}[!h]
\caption{{\bf Peak RAM Usage for QIIME (\textit{de novo} OTU picking) and Amquery pipelines for sets of samples of different size.}
A, B: Peak RAM usage during \textit{de novo} OTU picking and beta-diversity analysis by QIIME using Bray-Curtis similarity (A) and weighted Unifrac (B). 
C: Peak RAM usage during sample indexing by Amquery, the same as Fig~\ref{fig3}.C subfigure in the scale of A and B measurements.}
\label{fig4}
\end{figure}



\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{{\bf Peak RAM usage of Amquery and reference-based QIIME pipelines }}
\begin{tabular}{|l+l|l|l|l|l|l|l|}
\hline
\textit{size} & {\bf Amquery} & \multicolumn{6}{|l|}{\bf Reference-based QIIME}\\ \hline
& {\bf Peak} & pick\_otus & make\_otu & bdiv\_wu & bdiv\_bc & {\bf Peak (wu)} & {\bf Peak (bc)}  \\ \thickhline
100 & 100Mb & 242Mb & 120Mb & 145Mb & 135Mb & 242Mb & 242Mb \\ \hline
200 & 149Mb & 344Mb & 130Mb & 209Mb & 197Mb & 344Mb & 344Mb \\ \hline
300 & 222Mb & 437Mb & 135Mb & 295Mb & 275Mb & 437Mb & 437Mb \\ \hline
400 & 229Mb & 499Mb & 139Mb & 387Mb & 366Mb & 499Mb & 499Mb \\ \hline
500 & 290Mb & 649Mb & 146Mb & 541Mb & 516Mb & 649Mb & 649Mb \\ \hline
600 & 334Mb & 759Mb & 153Mb & 716Mb & 686Mb & 759Mb & 759Mb \\ \hline
700 & 379Mb & 833Mb & 163Mb & 881Mb & 850Mb & 881Mb & 850Mb \\ \hline
800 & 428Mb & 911Mb & 168Mb & 1.0Gb & 1.0Gb & 1.0Gb & 1.0Gb \\ \hline
900 & 474Mb & 995Mb & 175Mb & 1.3Gb & 1.2Gb & 1.3Gb & 1.2Gb \\ \hline
1000 & 515Mb & 1.0Gb & 183Mb & 1.4Gb & 1.4Gb & 1.4Gb & 1.4Gb \\ \hline
\end{tabular}
\begin{flushleft}
Column naming notation corresponds to the Table~\ref{table1}. \textit{Peak} columns present the peak RAM usage of corresponding pipelines.   
\end{flushleft}
\label{table3}
\end{adjustwidth}
\end{table}



\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{{\bf Peak RAM usage of \textit{de novo} based QIIME pipeline }}
\begin{tabular}{|l+l|l|l|l|l|l|l|l|l|l|}
\hline
\textit{size} & \multicolumn{10}{|l|}{\bf \textit{De novo} based QIIME}\\ \hline
& pick\_otus & rep\_set & align & filt\_aln & make\_phyl & make\_otu & bdiv\_wu & bdiv\_bc & {\bf Peak (wu)} & {\bf Peak (bc)}  \\ \thickhline

100 & 174Mb & 174Mb & 298Mb & 166Mb & 149Mb & 118Mb & 169Mb & 132Mb & 298Mb & 298Mb \\ \hline
200 & 250Mb & 234Mb & 424Mb & 211Mb & 173Mb & 123Mb & 259Mb & 184Mb & 424Mb & 424Mb \\ \hline
300 & 345Mb & 295Mb & 578Mb & 269Mb & 205Mb & 131Mb & 407Mb & 264Mb & 578Mb & 578Mb \\ \hline
400 & 404Mb & 356Mb & 638Mb & 291Mb & 281Mb & 137Mb & 541Mb & 353Mb & 638Mb & 638Mb \\ \hline
500 & 545Mb & 420Mb & 822Mb & 363Mb & 377Mb & 142Mb & 793Mb & 492Mb & 822Mb & 822Mb \\ \hline
600 & 645Mb & 476Mb & 976Mb & 422Mb & 462Mb & 151Mb & 1Gb & 650Mb & 1Gb & 976Mb\\ \hline
700 & 716Mb & 553Mb & 1.0Gb & 445Mb & 500Mb & 161Mb & 1.3Gb & 803Mb & 1.3Gb & 1.0Gb\\ \hline
800 & 791Mb & 607Mb & 1.1Gb & 478Mb & 548Mb & 167Mb & 1.6Gb & 981Mb & 1.6Gb & 1.1Gb\\ \hline
900 & 871Mb & 666Mb & 1.3Gb & 548Mb & 570Mb & 174Mb & 2.Gb & 1.1Gb & 2.Gb & 1.3Gb\\ \hline
1000 & 903Mb & 726Mb & 1.4Gb & 568Mb & 676Mb & 179Mb & 2.2Gb & 1.3Gb & 2.2Gb & 1.4Gb \\ \hline

\end{tabular}
\begin{flushleft}
Column namings correspond to the tables above.   
\end{flushleft}
\label{table4}
\end{adjustwidth}
\end{table}




\subsection*{Similarity search effectiveness}
To estimate relevance of similarity search, provided by Amquery, mean precision at k (MP@K), mean average precision at k (MAP@K), and normalized cumulative discounted gain at k (NCDG@k) \cite{book, jarvelin2000ir, jarvelin2002cumulated}
are evaluated, with the relevance function, declared as follows. For every search query $q$, all the pairwise distances $D(q, x)$ provided by QIIME pipeline from $q$ to every sample $x$ in the index, are normalized. 
Thus, resulting relevance value $R(q, r)$ for search output result $r$ is the value $1 - \hat{D}(q, r)$, whree $\hat{D}$ is the normalized value of $D(q, x)$ over all the $x$ in the index. Here we use the notation of $D(q, r)$ for weighted Unifrac and Bray-Curtis similarity distances.

All the metrics are evaluated among all the dataset splits, mentioned above, for search query size $k = {1, 3, 5, 7, 10, 15, 20}$ and 100 random samples from the index. Since precision metrics as MP@k and MAP@k require binary relevance function,
$R(q, r)$ values for these metrics are binarized as follows:

\[ R_b(q, r)  =
  \begin{cases}
    1  & r \text{ in top-} k \text{ nearest samples of } q, \text{ according to } D(q, r)\\
    0  &  otherwise \\
  \end{cases}
\] 

MP@k, MAP@k, NDCG@k evaluation results presented in the Fig~\ref{fig5}.


\begin{figure}[!h]
\caption{{\bf MP@k (A, D), MAP@k (B, E) and NDCG@k (C, F) for Amquery search queries and random sampling from the index as a baseline, according to relevance functions, based on the reference-based QIIME pipeline result.}
A, B, C are calculated according to weighted Unifrac, while D, E, F are calculated according to Bray-Curtis similarity as a relevance function.}
\label{fig5}
\end{figure}


\subsection*{Conclusions}
We presented a k-mer based approach for fast indexing and searching of 16S amplicon libraries, which designed to outperform clustering-based solutions for similarity search, in terms of speed and memory consumption.
This approach allows for indexing thousands of samples in minutes, and lightning fast similarity search. We demonstrated that similarity search results, based on RJSD distance over estimated k-mer distributions, correspond to a certain extent to the weighted Unifrac based and Bray-Curtis similarity based search results.


\begin{thebibliography}{10}

\bibitem{caporaso2010qiime}
J~Gregory Caporaso, Justin Kuczynski, Jesse Stombaugh, Kyle Bittinger,
  Frederic~D Bushman, Elizabeth~K Costello, Noah Fierer, Antonio~Gonzalez
  Pe{\~n}a, Julia~K Goodrich, Jeffrey~I Gordon, et~al.
\newblock Qiime allows analysis of high-throughput community sequencing data.
\newblock {\em Nature methods}, 7(5):335--336, 2010.

\bibitem{chavez2001searching}
Edgar Ch{\'a}vez, Gonzalo Navarro, Ricardo Baeza-Yates, and Jos{\'e}~Luis
  Marroqu{\'\i}n.
\newblock Searching in metric spaces.
\newblock {\em ACM computing surveys (CSUR)}, 33(3):273--321, 2001.

\bibitem{book}
Prabhakar~Raghavan Christopher D~Manning and Hinrich Schütze.
\newblock {\em Introduction to Information Retrieval}.
\newblock Cambridge University Press, 2008.

\bibitem{connor2013evaluation}
Richard Connor, Franco~Alberto Cardillo, Robert Moss, and Fausto Rabitti.
\newblock Evaluation of jensen-shannon distance over sparse data.
\newblock In {\em International Conference on Similarity Search and
  Applications}, pages 163--168. Springer, 2013.

\bibitem{endres2003new}
Dominik~Maria Endres and Johannes~E Schindelin.
\newblock A new metric for probability distributions.
\newblock {\em IEEE Transactions on Information theory}, 49(7):1858--1860,
  2003.

\bibitem{fuglede2004jensen}
Bent Fuglede and Flemming Topsoe.
\newblock Jensen-shannon divergence and hilbert space embedding.
\newblock In {\em Information Theory, 2004. ISIT 2004. Proceedings.
  International Symposium on}, page~31. IEEE, 2004.

\bibitem{jarvelin2000ir}
Kalervo J{\"a}rvelin and Jaana Kek{\"a}l{\"a}inen.
\newblock Ir evaluation methods for retrieving highly relevant documents.
\newblock In {\em Proceedings of the 23rd annual international ACM SIGIR
  conference on Research and development in information retrieval}, pages
  41--48. ACM, 2000.

\bibitem{jarvelin2002cumulated}
Kalervo J{\"a}rvelin and Jaana Kek{\"a}l{\"a}inen.
\newblock Cumulated gain-based evaluation of ir techniques.
\newblock {\em ACM Transactions on Information Systems (TOIS)}, 20(4):422--446,
  2002.

\bibitem{karp1987efficient}
Richard~M Karp and Michael~O Rabin.
\newblock Efficient randomized pattern-matching algorithms.
\newblock {\em IBM Journal of Research and Development}, 31(2):249--260, 1987.

\bibitem{leinonen2010sequence}
Rasko Leinonen, Hideaki Sugawara, and Martin Shumway.
\newblock The sequence read archive.
\newblock {\em Nucleic acids research}, page gkq1019, 2010.

\bibitem{ligi2014characterization}
Teele Ligi, Kristjan Oopkaup, Marika Truu, Jens-Konrad Preem, Hiie N{\~o}lvak,
  William~J Mitsch, {\"U}lo Mander, and Jaak Truu.
\newblock Characterization of bacterial communities in soil and sediment of a
  created riverine wetland complex using high-throughput 16s rrna amplicon
  sequencing.
\newblock {\em Ecological Engineering}, 72:56--66, 2014.

\bibitem{lin1991divergence}
Jianhua Lin.
\newblock Divergence measures based on the shannon entropy.
\newblock {\em IEEE Transactions on Information theory}, 37(1):145--151, 1991.

\bibitem{lozupone2011unifrac}
Catherine Lozupone, Manuel~E Lladser, Dan Knights, Jesse Stombaugh, and Rob
  Knight.
\newblock Unifrac: an effective distance metric for microbial community
  comparison.
\newblock {\em The ISME journal}, 5(2):169, 2011.

\bibitem{qin2010human}
Junjie Qin, Ruiqiang Li, Jeroen Raes, Manimozhiyan Arumugam,
  Kristoffer~Solvsten Burgdorf, Chaysavanh Manichanh, Trine Nielsen, Nicolas
  Pons, Florence Levenez, Takuji Yamada, et~al.
\newblock A human gut microbial gene catalogue established by metagenomic
  sequencing.
\newblock {\em nature}, 464(7285):59--65, 2010.

\bibitem{seedorf2014bacteria}
Henning Seedorf, Nicholas~W Griffin, Vanessa~K Ridaura, Alejandro Reyes, Jiye
  Cheng, Federico~E Rey, Michelle~I Smith, Gabriel~M Simon, Rudolf~H
  Scheffrahn, Dagmar Woebken, et~al.
\newblock Bacteria from diverse habitats colonize and compete in the mouse gut.
\newblock {\em Cell}, 159(2):253--266, 2014.

\bibitem{yianilos1993data}
Peter~N Yianilos.
\newblock Data structures and algorithms for nearest neighbor search in general
  metric spaces.
\newblock In {\em SODA}, volume~93, pages 311--21, 1993.

\end{thebibliography}



\end{document}

